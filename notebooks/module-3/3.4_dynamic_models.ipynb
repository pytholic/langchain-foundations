{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7053c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4424cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "large_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "standard_model = ChatOllama(\n",
    "    model=\"gemma2:2b\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "@wrap_model_call\n",
    "def state_based_model(request: ModelRequest, \n",
    "handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
    "    \"\"\"Select model based on State conversation length.\"\"\"\n",
    "    # request.messages is a shortcut for request.state[\"messages\"]\n",
    "    message_count = len(request.messages)  \n",
    "\n",
    "    if message_count > 10:\n",
    "        # Long conversation - use model with larger context window\n",
    "        model = large_model\n",
    "    else:\n",
    "        # Short conversation - use efficient model\n",
    "        model = standard_model\n",
    "\n",
    "    request = request.override(model=model)  \n",
    "\n",
    "    return handler(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be519e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=small_model,\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"You are roleplaying a real life helpful office intern.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e97db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*I glance at the office plant, which is looking rather wilted.*  \"Uh, no! I'm so sorry, I completely forgot.  My apologies!  Do you want me to grab a watering can and give it a good soak?\" \n",
      "\n",
      "*I offer a sheepish smile.* \"Is there anything else I can help with today? Maybe I could make some coffee for the team?\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3d981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma2:2b\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a30e9c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I think I might have accidentally over-watered it last time! ðŸŒ¿  Do you want to check if there's anything else I can do for it? ðŸ˜Š \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\"),\n",
    "        AIMessage(content=\"Yes, I gave it a light watering this morning.\"),\n",
    "        HumanMessage(content=\"Has it grown much this week?\"),\n",
    "        AIMessage(content=\"It's sprouted two new leaves since Monday.\"),\n",
    "        HumanMessage(content=\"Are the leaves still turning yellow on the edges?\"),\n",
    "        AIMessage(content=\"A little, but it's looking healthier overall.\"),\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86262d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma2:2b\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53dc41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on its current growth rate, we'll probably need to consider repotting it sometime next spring.\n",
      "\n",
      "We'll know it's definitely time when we see roots starting to come out of the drainage holes, or if it looks like the plant is getting too top-heavy for its current pot. I'll keep an eye on it!\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\"),\n",
    "        AIMessage(content=\"Yes, I gave it a light watering this morning.\"),\n",
    "        HumanMessage(content=\"Has it grown much this week?\"),\n",
    "        AIMessage(content=\"It's sprouted two new leaves since Monday.\"),\n",
    "        HumanMessage(content=\"Are the leaves still turning yellow on the edges?\"),\n",
    "        AIMessage(content=\"A little, but it's looking healthier overall.\"),\n",
    "        HumanMessage(content=\"Did you remember to rotate the pot toward the window?\"),\n",
    "        AIMessage(content=\"I rotated it a quarter turn so it gets more even light.\"),\n",
    "        HumanMessage(content=\"How often should we be fertilizing this plant?\"),\n",
    "        AIMessage(content=\"About once every two weeks with a diluted liquid fertilizer.\"),\n",
    "        HumanMessage(content=\"When should we expect to have to replace the pot?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f25a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb51139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
